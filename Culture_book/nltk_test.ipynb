{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f98c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Hannanum\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5b7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(df):\n",
    "    # 1. 저자와 책 제목을 title 컬럼으로 만들어줌 (띄어쓰기로 구분)\n",
    "    df = df[df['TITLE_NM'].str.contains(\"[\\uac00-\\ud7a3\\u3131-\\u3163\\uac01-\\ud7a6]+\")] # 한국어 외 삭제\n",
    "    df['TITLE_NM'] = df['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x)) # 숫자 외 특수문자 제거\n",
    "\n",
    "    # 2. TITLE_NM의 양쪽 공백 제거\n",
    "    df['TITLE_NM'] = df['TITLE_NM'].str.strip()\n",
    "\n",
    "    # 3. 중복 값 삭제\n",
    "    df = df.drop_duplicates(subset='TITLE_NM')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2c37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "        print(b_title)\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7da104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maek_vector_data(data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    return data_1, vectorizer1, vectorized_data1, data_2, vectorizer2, vectorized_data2, data_3, vectorizer3, vectorized_data3,\\\n",
    "            data_4, vectorizer4, vectorized_data4, data_5, vectorizer5, vectorized_data5, data_6, vectorizer6, vectorized_data6,\\\n",
    "            data_7, vectorizer7, vectorized_data7,data_8, vectorizer8, vectorized_data8,data_9, vectorizer9, vectorized_data9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10fe3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(test_data, data_1, vectorizer1,vectorized_data1,\\\n",
    "              data_2, vectorizer2,vectorized_data2, data_3, vectorizer3,vectorized_data3,\\\n",
    "              data_4, vectorizer4,vectorized_data4, data_5, vectorizer5,vectorized_data5,\\\n",
    "              data_6, vectorizer6,vectorized_data6, data_7, vectorizer7,vectorized_data7,\\\n",
    "              data_8, vectorizer8,vectorized_data8, data_9, vectorizer9, vectorized_data9, n=20):\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        print(f\"{num}월\")\n",
    "        similar_titles_list = []\n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        similar_titles_list = [item for sublist in similar_titles_list for item in sublist]\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64aac9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>AUTHR_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>COUNTING</th>\n",
       "      <th>LBRRY_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>세대</td>\n",
       "      <td>진 트웬지 지음 ;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>법률가</td>\n",
       "      <td>김동일 ;어윤경 ;최윤정 지음</td>\n",
       "      <td>372.68</td>\n",
       "      <td>8</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>페미니즘 그녀들의이야기</td>\n",
       "      <td>김효진 지음</td>\n",
       "      <td>809.9</td>\n",
       "      <td>2</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음</td>\n",
       "      <td>331.233</td>\n",
       "      <td>1</td>\n",
       "      <td>4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>30002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TITLE_NM          AUTHR_NM   KDC_NM  COUNTING  \\\n",
       "0                               세대  진 트웬지 지음 ;김현정 옮김  331.233         6   \n",
       "1                              법률가  김동일 ;어윤경 ;최윤정 지음   372.68         8   \n",
       "10                    페미니즘 그녀들의이야기            김효진 지음    809.9         2   \n",
       "11  세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기          진 트웬지 지음  331.233         1   \n",
       "12      세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기   진 트웬지 지음;김현정 옮김  331.233         6   \n",
       "\n",
       "    LBRRY_CD  \n",
       "0       5300  \n",
       "1       5301  \n",
       "10      8400  \n",
       "11      4504  \n",
       "12     30002  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b075b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>AUTHR_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>LBRRY_NM</th>\n",
       "      <th>LOAN_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미키7 : 애드워드 애슈턴 SF 장편소설</td>\n",
       "      <td>애드워드 애슈턴 지음 ; 배지혜 옮김</td>\n",
       "      <td>843.6-애56ㅁ</td>\n",
       "      <td>[스마트]양천중앙</td>\n",
       "      <td>22/12/31 21:45:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마흔, 부부가 함께 은퇴합니다 : 5년 만에 40대 조기 은퇴에 성공한, 금융맹 부...</td>\n",
       "      <td>김다현 지음</td>\n",
       "      <td>327.04-김22ㅁ</td>\n",
       "      <td>[스마트]양천25시(오목교역)</td>\n",
       "      <td>22/12/31 21:15:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(조셉 필라테스의)필라테스 바이블</td>\n",
       "      <td>조셉 필라테스,저드 로빈스,린 반 휴트-로빈스 [공]엮음 ; 원정희 옮김</td>\n",
       "      <td>517.32-필292ㅍ</td>\n",
       "      <td>[스마트] 신정네거리</td>\n",
       "      <td>22/12/31 20:51:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>헤어질 결심 각본</td>\n",
       "      <td>정서경,박찬욱 지음</td>\n",
       "      <td>812.66-정54ㅎ</td>\n",
       "      <td>[스마트]양천25시(오목교역)</td>\n",
       "      <td>22/12/31 17:36:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너무 잘하려고 애쓰지 마라</td>\n",
       "      <td>나태주 지음</td>\n",
       "      <td>811.6-나883너</td>\n",
       "      <td>[스마트]양천중앙</td>\n",
       "      <td>22/12/31 17:35:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TITLE_NM  \\\n",
       "0                             미키7 : 애드워드 애슈턴 SF 장편소설   \n",
       "1  마흔, 부부가 함께 은퇴합니다 : 5년 만에 40대 조기 은퇴에 성공한, 금융맹 부...   \n",
       "2                                 (조셉 필라테스의)필라테스 바이블   \n",
       "3                                          헤어질 결심 각본   \n",
       "4                                     너무 잘하려고 애쓰지 마라   \n",
       "\n",
       "                                   AUTHR_NM        KDC_NM          LBRRY_NM  \\\n",
       "0                      애드워드 애슈턴 지음 ; 배지혜 옮김    843.6-애56ㅁ         [스마트]양천중앙   \n",
       "1                                    김다현 지음   327.04-김22ㅁ  [스마트]양천25시(오목교역)   \n",
       "2  조셉 필라테스,저드 로빈스,린 반 휴트-로빈스 [공]엮음 ; 원정희 옮김  517.32-필292ㅍ       [스마트] 신정네거리   \n",
       "3                                정서경,박찬욱 지음   812.66-정54ㅎ  [스마트]양천25시(오목교역)   \n",
       "4                                    나태주 지음   811.6-나883너         [스마트]양천중앙   \n",
       "\n",
       "           LOAN_DATE  \n",
       "0  22/12/31 21:45:38  \n",
       "1  22/12/31 21:15:56  \n",
       "2  22/12/31 20:51:24  \n",
       "3  22/12/31 17:36:14  \n",
       "4  22/12/31 17:35:47  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yangchon = pd.read_csv(\"./test_data/TEST_YANGCHEON.csv\")\n",
    "yangchon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "202d9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1, vc1, vd1, da2, vc2, vd2, da3, vc3, vd3,da4, vc4, vd4, da5, vc5, vd5, da6, vc6, vd6, da7, vc7, vd7, da8, vc8, vd8, da9, vc9, vd9 = maek_vector_data(data, n= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2991bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1월\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_score \u001b[38;5;241m=\u001b[39m \u001b[43mmake_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43myangchon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvc9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd9\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 134\u001b[0m, in \u001b[0;36mmake_score\u001b[1;34m(test_data, data_1, vectorizer1, vectorized_data1, data_2, vectorizer2, vectorized_data2, data_3, vectorizer3, vectorized_data3, data_4, vectorizer4, vectorized_data4, data_5, vectorizer5, vectorized_data5, data_6, vectorizer6, vectorized_data6, data_7, vectorizer7, vectorized_data7, data_8, vectorizer8, vectorized_data8, data_9, vectorizer9, vectorized_data9, n)\u001b[0m\n\u001b[0;32m    132\u001b[0m after_book \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_data[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOAN_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mnum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE_NM\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    133\u001b[0m similar_titles_list \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m similar_titles_list \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[1;32m--> 134\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafter_book\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilar_titles_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m월 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m score_list\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(a_list, b_list)\u001b[0m\n\u001b[0;32m      5\u001b[0m a_tokens \u001b[38;5;241m=\u001b[39m [hannanum\u001b[38;5;241m.\u001b[39mmorphs(title) \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m a_list]\n\u001b[0;32m      6\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 7\u001b[0m a_vectorized \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_title \u001b[38;5;129;01min\u001b[39;00m b_list:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(b_title)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nltk_setting\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nltk_setting\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "result_score = make_score(yangchon, da1, vc1, vd1, da2, vc2, vd2, da3, vc3, vd3, da4, vc4, vd4, da5, vc5, vd5, da6, vc6, vd6, da7, vc7, vd7, da8, vc8, vd8, da9, vc9, vd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcb403",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02c917",
   "metadata": {},
   "source": [
    "# 양천구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c0e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity\n",
    "\n",
    "def maek_score(data, test_data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        similar_titles_list = []\n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        similar_titles_list = [item for sublist in similar_titles_list for item in sublist]\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num+1}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "370bd340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>AUTHR_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>COUNTING</th>\n",
       "      <th>LBRRY_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>세대</td>\n",
       "      <td>진 트웬지 지음 ;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>법률가</td>\n",
       "      <td>김동일 ;어윤경 ;최윤정 지음</td>\n",
       "      <td>372.68</td>\n",
       "      <td>8</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>페미니즘 그녀들의이야기</td>\n",
       "      <td>김효진 지음</td>\n",
       "      <td>809.9</td>\n",
       "      <td>2</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음</td>\n",
       "      <td>331.233</td>\n",
       "      <td>1</td>\n",
       "      <td>4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>30002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TITLE_NM          AUTHR_NM   KDC_NM  COUNTING  \\\n",
       "0                               세대  진 트웬지 지음 ;김현정 옮김  331.233         6   \n",
       "1                              법률가  김동일 ;어윤경 ;최윤정 지음   372.68         8   \n",
       "10                    페미니즘 그녀들의이야기            김효진 지음    809.9         2   \n",
       "11  세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기          진 트웬지 지음  331.233         1   \n",
       "12      세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기   진 트웬지 지음;김현정 옮김  331.233         6   \n",
       "\n",
       "    LBRRY_CD  \n",
       "0       5300  \n",
       "1       5301  \n",
       "10      8400  \n",
       "11      4504  \n",
       "12     30002  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "425c4908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>AUTHR_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>LBRRY_NM</th>\n",
       "      <th>LOAN_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미키7 : 애드워드 애슈턴 SF 장편소설</td>\n",
       "      <td>애드워드 애슈턴 지음 ; 배지혜 옮김</td>\n",
       "      <td>843.6-애56ㅁ</td>\n",
       "      <td>[스마트]양천중앙</td>\n",
       "      <td>22/12/31 21:45:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마흔, 부부가 함께 은퇴합니다 : 5년 만에 40대 조기 은퇴에 성공한, 금융맹 부...</td>\n",
       "      <td>김다현 지음</td>\n",
       "      <td>327.04-김22ㅁ</td>\n",
       "      <td>[스마트]양천25시(오목교역)</td>\n",
       "      <td>22/12/31 21:15:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(조셉 필라테스의)필라테스 바이블</td>\n",
       "      <td>조셉 필라테스,저드 로빈스,린 반 휴트-로빈스 [공]엮음 ; 원정희 옮김</td>\n",
       "      <td>517.32-필292ㅍ</td>\n",
       "      <td>[스마트] 신정네거리</td>\n",
       "      <td>22/12/31 20:51:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>헤어질 결심 각본</td>\n",
       "      <td>정서경,박찬욱 지음</td>\n",
       "      <td>812.66-정54ㅎ</td>\n",
       "      <td>[스마트]양천25시(오목교역)</td>\n",
       "      <td>22/12/31 17:36:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너무 잘하려고 애쓰지 마라</td>\n",
       "      <td>나태주 지음</td>\n",
       "      <td>811.6-나883너</td>\n",
       "      <td>[스마트]양천중앙</td>\n",
       "      <td>22/12/31 17:35:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TITLE_NM  \\\n",
       "0                             미키7 : 애드워드 애슈턴 SF 장편소설   \n",
       "1  마흔, 부부가 함께 은퇴합니다 : 5년 만에 40대 조기 은퇴에 성공한, 금융맹 부...   \n",
       "2                                 (조셉 필라테스의)필라테스 바이블   \n",
       "3                                          헤어질 결심 각본   \n",
       "4                                     너무 잘하려고 애쓰지 마라   \n",
       "\n",
       "                                   AUTHR_NM        KDC_NM          LBRRY_NM  \\\n",
       "0                      애드워드 애슈턴 지음 ; 배지혜 옮김    843.6-애56ㅁ         [스마트]양천중앙   \n",
       "1                                    김다현 지음   327.04-김22ㅁ  [스마트]양천25시(오목교역)   \n",
       "2  조셉 필라테스,저드 로빈스,린 반 휴트-로빈스 [공]엮음 ; 원정희 옮김  517.32-필292ㅍ       [스마트] 신정네거리   \n",
       "3                                정서경,박찬욱 지음   812.66-정54ㅎ  [스마트]양천25시(오목교역)   \n",
       "4                                    나태주 지음   811.6-나883너         [스마트]양천중앙   \n",
       "\n",
       "           LOAN_DATE  \n",
       "0  22/12/31 21:45:38  \n",
       "1  22/12/31 21:15:56  \n",
       "2  22/12/31 20:51:24  \n",
       "3  22/12/31 17:36:14  \n",
       "4  22/12/31 17:35:47  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yangchon = pd.read_csv(\"./test_data/TEST_YANGCHEON.csv\")\n",
    "yangchon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f48b3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1월\n",
      "0.6263529033993118\n",
      "2월\n",
      "0.6526568817340849\n",
      "3월\n",
      "0.6415646037679946\n",
      "4월\n",
      "0.67673078819684\n",
      "5월\n",
      "0.6544180471523545\n",
      "6월\n",
      "0.6882998256452502\n",
      "7월\n",
      "0.7142331369081706\n",
      "8월\n",
      "0.6781380083285854\n",
      "9월\n",
      "0.6932941926034685\n",
      "10월\n",
      "0.6695793241208458\n",
      "11월\n",
      "0.6841502895938003\n"
     ]
    }
   ],
   "source": [
    "score_result = maek_score(data, yangchon, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd064486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708561819500642"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca62cf",
   "metadata": {},
   "source": [
    "# 동대문구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086dafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>AUTHR_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>COUNTING</th>\n",
       "      <th>LBRRY_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>세대</td>\n",
       "      <td>진 트웬지 지음 ;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>법률가</td>\n",
       "      <td>김동일 ;어윤경 ;최윤정 지음</td>\n",
       "      <td>372.68</td>\n",
       "      <td>8</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>페미니즘 그녀들의이야기</td>\n",
       "      <td>김효진 지음</td>\n",
       "      <td>809.9</td>\n",
       "      <td>2</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음</td>\n",
       "      <td>331.233</td>\n",
       "      <td>1</td>\n",
       "      <td>4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기</td>\n",
       "      <td>진 트웬지 지음;김현정 옮김</td>\n",
       "      <td>331.233</td>\n",
       "      <td>6</td>\n",
       "      <td>30002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TITLE_NM          AUTHR_NM   KDC_NM  COUNTING  \\\n",
       "0                               세대  진 트웬지 지음 ;김현정 옮김  331.233         6   \n",
       "1                              법률가  김동일 ;어윤경 ;최윤정 지음   372.68         8   \n",
       "10                    페미니즘 그녀들의이야기            김효진 지음    809.9         2   \n",
       "11  세대    스마트폰을 손에 쥐고 자란 요즘 세대 이야기          진 트웬지 지음  331.233         1   \n",
       "12      세대스마트폰을 손에 쥐고 자란 요즘 세대 이야기   진 트웬지 지음;김현정 옮김  331.233         6   \n",
       "\n",
       "    LBRRY_CD  \n",
       "0       5300  \n",
       "1       5301  \n",
       "10      8400  \n",
       "11      4504  \n",
       "12     30002  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04dc460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_NM</th>\n",
       "      <th>KDC_NM</th>\n",
       "      <th>LOAN_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>요리코를 위해  : 노리즈키 린타로 장편소설</td>\n",
       "      <td>833.6-ㄴ65요=2</td>\n",
       "      <td>2022-02-22 18:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>피프티 피플  : 정세랑 장편소설</td>\n",
       "      <td>813.7-ㅈ416ㅍ=3</td>\n",
       "      <td>2022-02-22 18:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 환자</td>\n",
       "      <td>843.6-ㄷ96ㄱ=3</td>\n",
       "      <td>2022-02-19 10:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>방구석 미술관  : 가볍고 편하게 시작하는 유쾌한 교양 미술</td>\n",
       "      <td>650.4-ㅈ664ㅂ=4</td>\n",
       "      <td>2022-01-25 19:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나미야 잡화점의 기적  : 히가시노 게이고 장편소설</td>\n",
       "      <td>833.6-ㅎ961나=7</td>\n",
       "      <td>2022-04-04 12:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TITLE_NM         KDC_NM         LOAN_DATE\n",
       "0           요리코를 위해  : 노리즈키 린타로 장편소설   833.6-ㄴ65요=2  2022-02-22 18:47\n",
       "1                 피프티 피플  : 정세랑 장편소설  813.7-ㅈ416ㅍ=3  2022-02-22 18:46\n",
       "2                               그 환자   843.6-ㄷ96ㄱ=3  2022-02-19 10:55\n",
       "3  방구석 미술관  : 가볍고 편하게 시작하는 유쾌한 교양 미술  650.4-ㅈ664ㅂ=4  2022-01-25 19:09\n",
       "4       나미야 잡화점의 기적  : 히가시노 게이고 장편소설  833.6-ㅎ961나=7  2022-04-04 12:43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dongdea = pd.read_csv(\"./test_data/TEST_DONGDAEMOON.csv\")\n",
    "dongdea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8658f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2월 : 0.6192514648372907\n",
      "3월 : 0.6715690917296608\n",
      "4월 : 0.6622452679813952\n",
      "5월 : 0.6020393793561438\n",
      "6월 : 0.6073227314248151\n",
      "7월 : 0.5922360546364491\n",
      "8월 : 0.5967222832398418\n",
      "9월 : 0.5607355000428706\n",
      "10월 : 0.5963488559538573\n",
      "11월 : 0.5781447491207625\n",
      "12월 : 0.5923366272766477\n"
     ]
    }
   ],
   "source": [
    "score_result = maek_score(data, dongdea, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b267e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071774550545214"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8bc69",
   "metadata": {},
   "source": [
    "# 강동구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e01e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2월 : 0.7026851532290873\n",
      "3월 : 0.6924217399590676\n",
      "4월 : 0.6354555350483349\n",
      "5월 : 0.6806042201190383\n",
      "6월 : 0.6642059588244923\n",
      "7월 : 0.6711466176333449\n",
      "8월 : 0.6219613041053543\n",
      "9월 : 0.6466609295277334\n",
      "10월 : 0.6845148963499272\n",
      "11월 : 0.661296544535431\n",
      "12월 : 0.6180141549177234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6617242776590486"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity\n",
    "\n",
    "def maek_score(data, test_data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        similar_titles_list = []\n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        similar_titles_list = [item for sublist in similar_titles_list for item in sublist]\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num+1}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list\n",
    "\n",
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "\n",
    "\n",
    "gangdong = pd.read_csv(\"./test_data/TEST_GANGDONG.csv\")\n",
    "\n",
    "score_result = maek_score(data, gangdong, n=20)\n",
    "\n",
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbf20e",
   "metadata": {},
   "source": [
    "# 강남구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66712df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2월 : 0.6770875969114531\n",
      "3월 : 0.7502071727740767\n",
      "4월 : 0.36305839746643603\n",
      "5월 : 0.6303979221013379\n",
      "6월 : 0.5590355651119163\n",
      "7월 : 0.5661797201286826\n",
      "8월 : 0.528175645194381\n",
      "9월 : 0.6325646303965027\n",
      "10월 : 0.5979484438064724\n",
      "11월 : 0.572486736758772\n",
      "12월 : 0.5915265037174929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5880607576697748"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity\n",
    "\n",
    "def maek_score(data, test_data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        similar_titles_list = []\n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        similar_titles_list = [item for sublist in similar_titles_list for item in sublist]\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num+1}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list\n",
    "\n",
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "\n",
    "\n",
    "gangdong = pd.read_csv(\"./test_data/TEST_GANGNAM.csv\")\n",
    "\n",
    "score_result = maek_score(data, gangdong, n=20)\n",
    "\n",
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d8425",
   "metadata": {},
   "source": [
    "# 송파구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24d8465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2월 : 0.6380076936457438\n",
      "3월 : 0.6230188564432669\n",
      "4월 : 0.6375407316731632\n",
      "5월 : 0.6425150750625248\n",
      "6월 : 0.6929862076995085\n",
      "7월 : 0.6453832146272175\n",
      "8월 : 0.6540117477885614\n",
      "9월 : 0.6646618860070624\n",
      "10월 : 0.684262618581251\n",
      "11월 : 0.6914488974970077\n",
      "12월 : 0.70431880315255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.661650521107078"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity\n",
    "\n",
    "def maek_score(data, test_data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        similar_titles_list = []\n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        similar_titles_list = [item for sublist in similar_titles_list for item in sublist]\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num+1}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list\n",
    "\n",
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "\n",
    "\n",
    "songpa = pd.read_csv(\"./test_data/TEST_SONGPA.csv\")\n",
    "\n",
    "score_result = maek_score(data, songpa, n=20)\n",
    "\n",
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d661d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "phrase input should be string, not <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 265\u001b[0m\n\u001b[0;32m    260\u001b[0m data \u001b[38;5;241m=\u001b[39m make_preprocessing(data)\n\u001b[0;32m    263\u001b[0m songpa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test_data/TEST_SONGPA.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m score_result \u001b[38;5;241m=\u001b[39m \u001b[43mmaek_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msongpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m np\u001b[38;5;241m.\u001b[39mmean(score_result)\n",
      "Cell \u001b[1;32mIn[3], line 253\u001b[0m, in \u001b[0;36mmaek_score\u001b[1;34m(data, test_data, n)\u001b[0m\n\u001b[0;32m    250\u001b[0m similar_titles_list_9 \u001b[38;5;241m=\u001b[39m get_top_n_books(similar_titles_list_9, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    252\u001b[0m similar_titles_list \u001b[38;5;241m=\u001b[39m similar_titles_list_1 \u001b[38;5;241m+\u001b[39m similar_titles_list_2 \u001b[38;5;241m+\u001b[39m similar_titles_list_3 \u001b[38;5;241m+\u001b[39m similar_titles_list_4 \u001b[38;5;241m+\u001b[39m similar_titles_list_5 \u001b[38;5;241m+\u001b[39m similar_titles_list_6 \u001b[38;5;241m+\u001b[39m similar_titles_list_7 \u001b[38;5;241m+\u001b[39m similar_titles_list_8 \u001b[38;5;241m+\u001b[39m similar_titles_list_9\n\u001b[1;32m--> 253\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafter_book\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilar_titles_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m월 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    255\u001b[0m score_list\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(a_list, b_list)\u001b[0m\n\u001b[0;32m     13\u001b[0m a_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m a_tokens])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_title \u001b[38;5;129;01min\u001b[39;00m b_list:\n\u001b[1;32m---> 18\u001b[0m     b_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mhannanum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     b_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(b_tokens)])\n\u001b[0;32m     24\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(a_vectorized, b_vectorized)\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nltk_setting\\lib\\site-packages\\konlpy\\tag\\_hannanum.py:115\u001b[0m, in \u001b[0;36mHannanum.morphs\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nltk_setting\\lib\\site-packages\\konlpy\\tag\\_hannanum.py:96\u001b[0m, in \u001b[0;36mHannanum.pos\u001b[1;34m(self, phrase, ntags, flatten, join)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, ntags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    This tagger is HMM based, and calculates the probability of tags.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    :param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[43mvalidate_phrase_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ntags \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n\u001b[0;32m     99\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjhi\u001b[38;5;241m.\u001b[39msimplePos09(phrase)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nltk_setting\\lib\\site-packages\\konlpy\\tag\\_common.py:20\u001b[0m, in \u001b[0;36mvalidate_phrase_inputs\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"validate if phrase input is provided in str format\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    phrase (str): phrase input\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrase input should be string, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(phrase)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(phrase, basestring), msg\n",
      "\u001b[1;31mAssertionError\u001b[0m: phrase input should be string, not <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "def get_top_n_books(book_list, n):\n",
    "    book_counts = Counter(book_list)\n",
    "    top_books = book_counts.most_common(n)\n",
    "    return top_books\n",
    "\n",
    "\n",
    "def calculate_similarity(a_list, b_list):\n",
    "    max_similarity_list = []\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    a_tokens = [hannanum.morphs(title) for title in a_list]\n",
    "    vectorizer = CountVectorizer()\n",
    "    a_vectorized = vectorizer.fit_transform([' '.join(tokens) for tokens in a_tokens])\n",
    "\n",
    "\n",
    "    for b_title in b_list:\n",
    "\n",
    "        b_tokens = hannanum.morphs(b_title)\n",
    "\n",
    "\n",
    "        b_vectorized = vectorizer.transform([' '.join(b_tokens)])\n",
    "    \n",
    "\n",
    "        similarity = cosine_similarity(a_vectorized, b_vectorized).max()\n",
    "        max_similarity_list.append(similarity)\n",
    "\n",
    "    average_similarity = sum(max_similarity_list) / len(max_similarity_list)\n",
    "    return average_similarity\n",
    "\n",
    "def maek_score(data, test_data, n = 20):\n",
    "    data[\"KDC_NM\"] = data[\"KDC_NM\"].astype(\"str\")\n",
    "\n",
    "    test_data['LOAN_DATE'] = pd.to_datetime(test_data['LOAN_DATE'])\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].dt.strftime('%m')\n",
    "    test_data['LOAN_DATE'] = test_data['LOAN_DATE'].astype('int')\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].apply(lambda x: re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', x))\n",
    "    test_data['TITLE_NM'] = test_data['TITLE_NM'].str.strip()\n",
    "    \n",
    "    data_1 = data[data[\"KDC_NM\"].str[0] == \"1\"]\n",
    "    data_1 = data_1.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_2 = data[data[\"KDC_NM\"].str[0] == \"2\"]\n",
    "    data_2 = data_2.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_3 = data[data[\"KDC_NM\"].str[0] == \"3\"]\n",
    "    data_3 = data_3.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_4 = data[data[\"KDC_NM\"].str[0] == \"4\"]\n",
    "    data_4 = data_4.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_5 = data[data[\"KDC_NM\"].str[0] == \"5\"]\n",
    "    data_5 = data_5.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_6 = data[data[\"KDC_NM\"].str[0] == \"6\"]\n",
    "    data_6 = data_6.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_7 = data[data[\"KDC_NM\"].str[0] == \"7\"]\n",
    "    data_7 = data_7.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_8 = data[data[\"KDC_NM\"].str[0] == \"8\"]\n",
    "    data_8 = data_8.sort_values(\"COUNTING\", ascending=False)\n",
    "    data_9 = data[data[\"KDC_NM\"].str[0] == \"9\"]\n",
    "    data_9 = data_9.sort_values(\"COUNTING\", ascending=False)\n",
    "    \n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    tokens1 = [hannanum.morphs(datum) for datum in data_1[\"TITLE_NM\"]]\n",
    "    tokens2 = [hannanum.morphs(datum) for datum in data_2[\"TITLE_NM\"]]\n",
    "    tokens3 = [hannanum.morphs(datum) for datum in data_3[\"TITLE_NM\"]]\n",
    "    tokens4 = [hannanum.morphs(datum) for datum in data_4[\"TITLE_NM\"]]\n",
    "    tokens5 = [hannanum.morphs(datum) for datum in data_5[\"TITLE_NM\"]]\n",
    "    tokens6 = [hannanum.morphs(datum) for datum in data_6[\"TITLE_NM\"]]\n",
    "    tokens7 = [hannanum.morphs(datum) for datum in data_7[\"TITLE_NM\"]]\n",
    "    tokens8 = [hannanum.morphs(datum) for datum in data_8[\"TITLE_NM\"]]\n",
    "    tokens9 = [hannanum.morphs(datum) for datum in data_9[\"TITLE_NM\"]]\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorized_data1 = vectorizer1.fit_transform([' '.join(token) for token in tokens1])\n",
    "    \n",
    "    vectorizer2 = CountVectorizer()\n",
    "    vectorized_data2 = vectorizer2.fit_transform([' '.join(token) for token in tokens2])\n",
    "    \n",
    "    vectorizer3 = CountVectorizer()\n",
    "    vectorized_data3 = vectorizer3.fit_transform([' '.join(token) for token in tokens3])\n",
    "    \n",
    "    vectorizer4 = CountVectorizer()\n",
    "    vectorized_data4 = vectorizer4.fit_transform([' '.join(token) for token in tokens4])\n",
    "    \n",
    "    vectorizer5 = CountVectorizer()\n",
    "    vectorized_data5 = vectorizer5.fit_transform([' '.join(token) for token in tokens5])\n",
    "    \n",
    "    vectorizer6 = CountVectorizer()\n",
    "    vectorized_data6 = vectorizer6.fit_transform([' '.join(token) for token in tokens6])\n",
    "    \n",
    "    vectorizer7 = CountVectorizer()\n",
    "    vectorized_data7 = vectorizer7.fit_transform([' '.join(token) for token in tokens7])\n",
    "    \n",
    "    vectorizer8 = CountVectorizer()\n",
    "    vectorized_data8 = vectorizer8.fit_transform([' '.join(token) for token in tokens8])\n",
    "    \n",
    "    vectorizer9 = CountVectorizer()\n",
    "    vectorized_data9 = vectorizer9.fit_transform([' '.join(token) for token in tokens9])\n",
    "    \n",
    "    score_list = []\n",
    "    for num in range(1, 12):\n",
    "        similar_titles_list_1 = []\n",
    "        similar_titles_list_2 = []\n",
    "        similar_titles_list_3 = []\n",
    "        similar_titles_list_4 = []\n",
    "        similar_titles_list_5 = []\n",
    "        similar_titles_list_6 = []\n",
    "        similar_titles_list_7 = []\n",
    "        similar_titles_list_8 = []\n",
    "        similar_titles_list_9 = []\n",
    "        \n",
    "        month_data = test_data[test_data['LOAN_DATE']==num]\n",
    "        new_title_list = list(month_data['TITLE_NM'])\n",
    "        kdc_list = list(month_data['KDC_NM'].str[0])\n",
    "        for kdc, new_title in zip(kdc_list, new_title_list):\n",
    "            if kdc == \"1\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer1.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data1, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_1.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_1.append(similar_titles)\n",
    "            elif kdc == \"2\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer2.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data2, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_2.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_2.append(similar_titles)\n",
    "            elif kdc == \"3\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer3.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data3, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "        \n",
    "                similar_books = data_3.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_3.append(similar_titles)\n",
    "            elif kdc == \"4\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer4.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data4, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_4.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_4.append(similar_titles)\n",
    "            elif kdc == \"5\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer5.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data5, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_5.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_5.append(similar_titles)\n",
    "            elif kdc == \"6\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer6.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data6, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_6.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_6.append(similar_titles)\n",
    "            elif kdc == \"7\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer7.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "    \n",
    "                similarities = cosine_similarity(vectorized_data7, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_7.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_7.append(similar_titles)\n",
    "            elif kdc == \"8\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer8.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "\n",
    "                similarities = cosine_similarity(vectorized_data8, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_8.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(10)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_8.append(similar_titles)\n",
    "            elif kdc == \"9\":\n",
    "                new_title = new_title\n",
    "                new_title_tokenized = hannanum.morphs(new_title)\n",
    "                new_title_vectorized = vectorizer9.transform([' '.join(new_title_tokenized)]).toarray()\n",
    "        \n",
    "                similarities = cosine_similarity(vectorized_data9, new_title_vectorized)\n",
    "                similar_indices = similarities.flatten().argsort()[-n:][::-1]\n",
    "    \n",
    "                similar_books = data_9.iloc[similar_indices]\n",
    "                similar_books = similar_books.sort_values(\"COUNTING\", ascending=False).head(5)\n",
    "                similar_titles = similar_books[\"TITLE_NM\"].tolist()\n",
    "                similar_titles_list_9.append(similar_titles)\n",
    "            else:\n",
    "                pass\n",
    "        after_book = list(test_data[test_data['LOAN_DATE']==num+1]['TITLE_NM'])\n",
    "        \n",
    "        similar_titles_list_1 = [item for sublist in similar_titles_list_1 for item in sublist]\n",
    "        similar_titles_list_1 = get_top_n_books(similar_titles_list_1, 5)\n",
    "        \n",
    "        similar_titles_list_2 = [item for sublist in similar_titles_list_2 for item in sublist]\n",
    "        similar_titles_list_2 = get_top_n_books(similar_titles_list_2, 5)\n",
    "        \n",
    "        similar_titles_list_3 = [item for sublist in similar_titles_list_3 for item in sublist]\n",
    "        similar_titles_list_3 = get_top_n_books(similar_titles_list_3, 5)\n",
    "        \n",
    "        similar_titles_list_4 = [item for sublist in similar_titles_list_4 for item in sublist]\n",
    "        similar_titles_list_4 = get_top_n_books(similar_titles_list_4, 5)\n",
    "        \n",
    "        similar_titles_list_5 = [item for sublist in similar_titles_list_5 for item in sublist]\n",
    "        similar_titles_list_5 = get_top_n_books(similar_titles_list_5, 5)\n",
    "        \n",
    "        similar_titles_list_6 = [item for sublist in similar_titles_list_6 for item in sublist]\n",
    "        similar_titles_list_6 = get_top_n_books(similar_titles_list_6, 5)\n",
    "        \n",
    "        similar_titles_list_7 = [item for sublist in similar_titles_list_7 for item in sublist]\n",
    "        similar_titles_list_7 = get_top_n_books(similar_titles_list_7, 5)\n",
    "        \n",
    "        similar_titles_list_8 = [item for sublist in similar_titles_list_8 for item in sublist]\n",
    "        similar_titles_list_8 = get_top_n_books(similar_titles_list_8, 10)\n",
    "        \n",
    "        similar_titles_list_9 = [item for sublist in similar_titles_list_9 for item in sublist]\n",
    "        similar_titles_list_9 = get_top_n_books(similar_titles_list_9, 5)\n",
    "        \n",
    "        similar_titles_list = similar_titles_list_1 + similar_titles_list_2 + similar_titles_list_3 + similar_titles_list_4 + similar_titles_list_5 + similar_titles_list_6 + similar_titles_list_7 + similar_titles_list_8 + similar_titles_list_9\n",
    "        result = calculate_similarity(after_book, similar_titles_list)\n",
    "        print(f\"{num+1}월 : {result}\")\n",
    "        score_list.append(result)\n",
    "    \n",
    "    return score_list\n",
    "\n",
    "data = pd.read_csv(\"./DataSet_row/BOOK_HIST.csv\")\n",
    "data = make_preprocessing(data)\n",
    "\n",
    "\n",
    "songpa = pd.read_csv(\"./test_data/TEST_SONGPA.csv\")\n",
    "\n",
    "score_result = maek_score(data, songpa, n=20)\n",
    "\n",
    "np.mean(score_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241bb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./test_data/TEST_SONGPA.csv\")\n",
    "df['LOAN_DATE'] = pd.to_datetime(df['LOAN_DATE'])\n",
    "df['LOAN_DATE'] = df['LOAN_DATE'].dt.strftime('%Y.%m.%d')\n",
    "df = df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd52860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : 생각이 너무 많은 서른 살에게 : 25년간 세계 최고의 인재들과 일하며 배운 것들 / kdc : 325.211-ㄱ873ㅅ-2 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 나보다 소중한 사람이 생겨버렸다  : 프레드릭 배크만 에세이 / kdc : 859.7-ㅂ682ㄴ / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 믿는 만큼 자라는 아이들 : 박혜란의 세 아들 이야기 / kdc : 598.104-ㅂ576ㅁ4 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 트렌드 코리아 2022  : 서울대 소비트렌드분석센터의 2022 전망 / kdc : 320.911-ㅌ94ㅁ-2022 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 파친코. 1 / kdc : 843-ㅇ733ㅍ-1-3 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 일의 격 : 성장하는 나, 성공하는 조직, 성숙한 삶 / kdc : 325.211-ㅅ856ㅇ-2 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 밀레니얼-Z세대 트렌드 2022  : 하나로 정의할 수 없는 MZ세대와 새로운 법칙을 만들어가는 Z세대 / kdc : 331.234-ㄷ51ㅁ-2022 / 날짜 : 2022.01.01\n",
      "\n",
      "제목 : 일기  : 황정은 에세이 / kdc : 814.7-ㅎ787ㅇ-2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 매우 예민한 사람들을 위한 책  : 뇌과학과 정신의학이 들려주는 당신 마음에 대한 이야기 / kdc : W화제 182.12-ㅈ336ㅁ-6 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 달러구트 꿈 백화점  : 주문하신 꿈은 매진입니다  : 이미예 장편소설 / kdc : 813.7-ㅇ732ㄷ-3 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 파리에서 도시락을 파는 여자  : 최정상으로 가는 7가지 부의 시크릿 / kdc : 325.211-ㅊ724ㅍ2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 컨버전스 2030 : 앞으로 10년 우리의 삶은 어떻게 바뀔 것인가?  : 미래의 부와 기회 / kdc : 321.97-ㄷ97ㅋ-2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 미적분의 쓸모  : 미래를 예측하는 새로운 언어 / kdc : 418.7-ㅎ395ㅁ-2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 채터, 당신 안의 훼방꾼  : 꼬리에 꼬리를 무는 생각과 거리 두는 기술 / kdc : 189.1-ㅋ816ㅊ-2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 행성어 서점  : 김초엽 짧은 소설 / kdc : 813.7-ㄱ958ㅎ / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 방금 떠나온 세계 : 김초엽 소설집 / kdc : 813.7-ㄱ958ㅂ / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 공간의 미래  : 코로나가 가속화시킨 공간 변화 / kdc : 540.04-ㅇ614고-2 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 체리새우  : 비밀글입니다 / kdc : W청 813.7-ㅎ767ㅊ-3 / 날짜 : 2022.01.02\n",
      "\n",
      "제목 : 걷는 독서  = Reading while walking along / kdc : 818-ㅂ224ㄱ-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 대한민국 재건축 재개발 지도 : 되는 곳만 골라 발 빠르게 투자하는 / kdc : 327.87-ㅈ488다-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 타이탄의 도구들  : 최고의 자리에 오른 사람들의 61가지 성공 비밀 / kdc : 325.211-ㅍ33ㅌ2-3 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 평생 공부력은 초5에 결정된다 / kdc : 375.2-ㅂ252ㅍ-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 나를 사랑하는 법 : 행복한 삶을 위해 나와 친해지기 / kdc : 834-ㅇ329나-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 스낵 인문학 : 간편하고 짤막하게 세상을 읽는 3분 지식 / kdc : 001.3-ㅌ25ㅅ-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 아름다운 사찰여행  : 인생에 쉼표가 필요하다면 산사로 가라 / kdc : 226.911-ㅇ599ㅇ-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 세계를 건너 너에게 갈게  : 이꽃님 장편소설 / kdc : W청 813.7-ㅇ682ㅅ-5 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 하버드 지혜 수업 : 78가지 사례로 배우는 행복과 성공을 위한 연금술 / kdc : 325.211-ㅁ692ㅎ / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : (생활견 키키와 반려인 진아의)오늘의 단어 / kdc : 818-ㅇ989오-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 오늘 밤, 세계에서 이 사랑이 사라진다 해도 / kdc : 833.6-ㅇ922옥-2 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 보건교사 안은영 : 정세랑 장편소설 / kdc : W화제 813.7-ㅈ416ㅂ2-4 / 날짜 : 2022.01.03\n",
      "\n",
      "제목 : 배움의 발견 : 나의 특별한 가족, 교육, 그리고 자유의 이야기 / kdc : 844-ㅇ556ㅂ-3 / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 사물의 뒷모습  : 안규철의 내 이야기로 그린 그림, 그 두 번째 이야기 / kdc : 814.7-ㅇ193ㅅ-2 / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 키스 더 유니버스 / kdc : 443.1-ㅋ416ㅋ / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 달러구트 꿈 백화점  : 이미예 장편소설. 2, 단골손님을 찾습니다 / kdc : 813.7-ㅇ732ㄷ-2-2 / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 퀸스 갬빗 / kdc : 843.6-ㅌ59ㅋ-2 / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 도서관 런웨이 : 윤고은 장편소설 / kdc : 813.7-ㅇ618ㄷ-2 / 날짜 : 2022.01.04\n",
      "\n",
      "제목 : 진이, 지니 : 정유정 장편소설 / kdc : W화제 813.7-ㅈ454ㅈ-5 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 아몬드  : 손원평 장편소설 / kdc : W청 813.7-ㅅ478ㅇ-6 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 라스트 듀얼  : 최후의 결투 / kdc : 843.6-ㅈ224ㄹ / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 달콤한 복수 주식회사 : 요나스 요나손 장편소설 / kdc : 859.7-ㅇ492ㄷ-2 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 다크 데이터 / kdc : 413-ㅎ439ㄷ-2 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 어른의 어휘력 : 말에 품격을 더하고 세상을 올바르게 이해하는 힘 / kdc : W화제 802.5-ㅇ577ㅇ-4 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 니체와 함께 산책을  : 세상의 속도에 휩쓸리지 않고 나를 여행하는 법 / kdc : 104-ㅅ754ㄴ / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : (서울 자가에 대기업 다니는)김 부장 이야기. 1, 김 부장 편 / kdc : 325.211-ㅅ626ㄱ-1-2 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 거꾸로 읽는 세계사 / kdc : 909-ㅇ582거 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 무언가 위험한 것이 온다  : 김희선 장편소설 / kdc : 813.7-ㄱ998무-2 / 날짜 : 2022.01.05\n",
      "\n",
      "제목 : 반야심경 마음공부 : 근심 걱정이 사라지고 인생이 편안해지는 / kdc : 223.53-ㅍ35ㅂ-2 / 날짜 : 2022.01.06\n",
      "\n",
      "제목 : 대치동 수학 공부의 비밀 / kdc : 373.4-ㄱ327ㄷ-2 / 날짜 : 2022.01.06\n",
      "\n",
      "제목 : (잘될 운명으로 가는) 운의 알고리즘 / kdc : 188.5-ㅈ528우-2 / 날짜 : 2022.01.06\n",
      "\n",
      "제목 : 헤맨다고 모두 길을 잃는 것은 아니다  : 김달 에세이 / kdc : 818-ㄱ692ㅎ-2 / 날짜 : 2022.01.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title, kdc, date_num in zip(df['TITLE_NM'], df['KDC_NM'], df['LOAN_DATE']):\n",
    "    print(f\"제목 : {title} / kdc : {kdc} / 날짜 : {date_num}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3bec337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQ_NO</th>\n",
       "      <th>BOOK_KEY_NO</th>\n",
       "      <th>MBER_SEQ_NO_VALUE</th>\n",
       "      <th>LON_DE</th>\n",
       "      <th>RTURN_DE</th>\n",
       "      <th>LBRRY_CD</th>\n",
       "      <th>LON_STLE_NM</th>\n",
       "      <th>RTURN_STLE_NM</th>\n",
       "      <th>RTURN_PREARNGE_DE</th>\n",
       "      <th>RESVE_DE</th>\n",
       "      <th>RESVE_END_DE</th>\n",
       "      <th>LON_STATE_NM</th>\n",
       "      <th>LON_PLACE_NM</th>\n",
       "      <th>RTURN_PLACE_NM</th>\n",
       "      <th>MANAGE_LBRRY_CD</th>\n",
       "      <th>LON_LBRRY_CD</th>\n",
       "      <th>RTURN_LBRRY_CD</th>\n",
       "      <th>MASTR_LBRRY_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48763587</td>\n",
       "      <td>12110127</td>\n",
       "      <td>4BD3A82F84C8235480772DC14C7F112F6EF0297C</td>\n",
       "      <td>2023-05-20 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31112</td>\n",
       "      <td>일반대출</td>\n",
       "      <td>대출중</td>\n",
       "      <td>2023-06-03 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>대출중</td>\n",
       "      <td>Eco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250000171569425</td>\n",
       "      <td>250000131728627</td>\n",
       "      <td>e3f149c80912a4efd0e7402857454fdd9afc8ad6f17cc2...</td>\n",
       "      <td>2021-03-12 12:00:00.0</td>\n",
       "      <td>2023-04-30 12:00:00.0</td>\n",
       "      <td>29902</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-03-26 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>KLAS</td>\n",
       "      <td>Flex_AMH</td>\n",
       "      <td>AB</td>\n",
       "      <td>AB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48763588</td>\n",
       "      <td>12110126</td>\n",
       "      <td>4BD3A82F84C8235480772DC14C7F112F6EF0297C</td>\n",
       "      <td>2023-05-20 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31112</td>\n",
       "      <td>일반대출</td>\n",
       "      <td>대출중</td>\n",
       "      <td>2023-06-03 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>대출중</td>\n",
       "      <td>Eco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250000306379574</td>\n",
       "      <td>250000283798976</td>\n",
       "      <td>0d3ad4481254933741ea24b79803c418849e9ddf93faaa...</td>\n",
       "      <td>2021-08-06 12:00:00.0</td>\n",
       "      <td>2023-04-30 12:00:00.0</td>\n",
       "      <td>29902</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-30 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>KLAS</td>\n",
       "      <td>Flex_AMH</td>\n",
       "      <td>AB</td>\n",
       "      <td>AB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48763589</td>\n",
       "      <td>12109966</td>\n",
       "      <td>4BD3A82F84C8235480772DC14C7F112F6EF0297C</td>\n",
       "      <td>2023-05-20 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31112</td>\n",
       "      <td>일반대출</td>\n",
       "      <td>대출중</td>\n",
       "      <td>2023-06-03 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>대출중</td>\n",
       "      <td>Eco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>JIN00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873425</th>\n",
       "      <td>128510284</td>\n",
       "      <td>124779702</td>\n",
       "      <td>048ad021a79ed5cfb3012644621f3eb86d2991ca071e89...</td>\n",
       "      <td>2023-04-16 12:00:00.0</td>\n",
       "      <td>2023-05-02 12:00:00.0</td>\n",
       "      <td>33500</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-05-01 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873426</th>\n",
       "      <td>128510878</td>\n",
       "      <td>124779944</td>\n",
       "      <td>10e1370f1c79adab65b6428633f7d3474ed7dcdb62592d...</td>\n",
       "      <td>2023-04-16 12:00:00.0</td>\n",
       "      <td>2023-05-02 12:00:00.0</td>\n",
       "      <td>33500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-01 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873427</th>\n",
       "      <td>128513546</td>\n",
       "      <td>72521383</td>\n",
       "      <td>6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...</td>\n",
       "      <td>2023-04-17 12:00:00.0</td>\n",
       "      <td>2023-05-01 12:00:00.0</td>\n",
       "      <td>33500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-08 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873428</th>\n",
       "      <td>128513548</td>\n",
       "      <td>124822423</td>\n",
       "      <td>6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...</td>\n",
       "      <td>2023-04-17 12:00:00.0</td>\n",
       "      <td>2023-05-01 12:00:00.0</td>\n",
       "      <td>33500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-08 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873429</th>\n",
       "      <td>128513550</td>\n",
       "      <td>125123906</td>\n",
       "      <td>6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...</td>\n",
       "      <td>2023-04-17 12:00:00.0</td>\n",
       "      <td>2023-05-11 12:00:00.0</td>\n",
       "      <td>33500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-08 12:00:00.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>KOLASIII</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1873430 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SEQ_NO      BOOK_KEY_NO  \\\n",
       "0               48763587         12110127   \n",
       "1        250000171569425  250000131728627   \n",
       "2               48763588         12110126   \n",
       "3        250000306379574  250000283798976   \n",
       "4               48763589         12109966   \n",
       "...                  ...              ...   \n",
       "1873425        128510284        124779702   \n",
       "1873426        128510878        124779944   \n",
       "1873427        128513546         72521383   \n",
       "1873428        128513548        124822423   \n",
       "1873429        128513550        125123906   \n",
       "\n",
       "                                         MBER_SEQ_NO_VALUE  \\\n",
       "0                 4BD3A82F84C8235480772DC14C7F112F6EF0297C   \n",
       "1        e3f149c80912a4efd0e7402857454fdd9afc8ad6f17cc2...   \n",
       "2                 4BD3A82F84C8235480772DC14C7F112F6EF0297C   \n",
       "3        0d3ad4481254933741ea24b79803c418849e9ddf93faaa...   \n",
       "4                 4BD3A82F84C8235480772DC14C7F112F6EF0297C   \n",
       "...                                                    ...   \n",
       "1873425  048ad021a79ed5cfb3012644621f3eb86d2991ca071e89...   \n",
       "1873426  10e1370f1c79adab65b6428633f7d3474ed7dcdb62592d...   \n",
       "1873427  6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...   \n",
       "1873428  6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...   \n",
       "1873429  6722c8f2dfd0321a1e0a5c09a7ddd0b5c287e58a76af44...   \n",
       "\n",
       "                        LON_DE               RTURN_DE  LBRRY_CD LON_STLE_NM  \\\n",
       "0        2023-05-20 12:00:00.0                    NaN     31112        일반대출   \n",
       "1        2021-03-12 12:00:00.0  2023-04-30 12:00:00.0     29902           1   \n",
       "2        2023-05-20 12:00:00.0                    NaN     31112        일반대출   \n",
       "3        2021-08-06 12:00:00.0  2023-04-30 12:00:00.0     29902           1   \n",
       "4        2023-05-20 12:00:00.0                    NaN     31112        일반대출   \n",
       "...                        ...                    ...       ...         ...   \n",
       "1873425  2023-04-16 12:00:00.0  2023-05-02 12:00:00.0     33500           0   \n",
       "1873426  2023-04-16 12:00:00.0  2023-05-02 12:00:00.0     33500           0   \n",
       "1873427  2023-04-17 12:00:00.0  2023-05-01 12:00:00.0     33500           0   \n",
       "1873428  2023-04-17 12:00:00.0  2023-05-01 12:00:00.0     33500           0   \n",
       "1873429  2023-04-17 12:00:00.0  2023-05-11 12:00:00.0     33500           0   \n",
       "\n",
       "        RTURN_STLE_NM      RTURN_PREARNGE_DE RESVE_DE RESVE_END_DE  \\\n",
       "0                 대출중  2023-06-03 12:00:00.0      NaN          NaN   \n",
       "1                   3  2021-03-26 12:00:00.0      NaN          NaN   \n",
       "2                 대출중  2023-06-03 12:00:00.0      NaN          NaN   \n",
       "3                   3  2022-04-30 12:00:00.0      NaN          NaN   \n",
       "4                 대출중  2023-06-03 12:00:00.0      NaN          NaN   \n",
       "...               ...                    ...      ...          ...   \n",
       "1873425           3.0  2023-05-01 12:00:00.0      NaN          NaN   \n",
       "1873426           0.0  2023-05-01 12:00:00.0      NaN          NaN   \n",
       "1873427           0.0  2023-05-08 12:00:00.0      NaN          NaN   \n",
       "1873428           0.0  2023-05-08 12:00:00.0      NaN          NaN   \n",
       "1873429           0.0  2023-05-08 12:00:00.0      NaN          NaN   \n",
       "\n",
       "        LON_STATE_NM LON_PLACE_NM RTURN_PLACE_NM MANAGE_LBRRY_CD LON_LBRRY_CD  \\\n",
       "0                대출중          Eco            NaN        JIN00000     JIN00000   \n",
       "1                  5         KLAS       Flex_AMH              AB           AB   \n",
       "2                대출중          Eco            NaN        JIN00000     JIN00000   \n",
       "3                  5         KLAS       Flex_AMH              AB           AB   \n",
       "4                대출중          Eco            NaN        JIN00000     JIN00000   \n",
       "...              ...          ...            ...             ...          ...   \n",
       "1873425            1     KOLASIII       KOLASIII              MA          NaN   \n",
       "1873426            1     KOLASIII       KOLASIII              MA          NaN   \n",
       "1873427            1     KOLASIII       KOLASIII              MA          NaN   \n",
       "1873428            1     KOLASIII       KOLASIII              MA          NaN   \n",
       "1873429            1     KOLASIII       KOLASIII              MA          NaN   \n",
       "\n",
       "        RTURN_LBRRY_CD  MASTR_LBRRY_CD  \n",
       "0                  NaN           31100  \n",
       "1                  NaN           29900  \n",
       "2                  NaN           31100  \n",
       "3                  NaN           29900  \n",
       "4                  NaN           31100  \n",
       "...                ...             ...  \n",
       "1873425            NaN           33500  \n",
       "1873426            NaN           33500  \n",
       "1873427            NaN           33500  \n",
       "1873428            NaN           33500  \n",
       "1873429            NaN           33500  \n",
       "\n",
       "[1873430 rows x 18 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./NL_CO_LOAN_PUB_202305-6.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ad69ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2675966155.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    2월 : 0.63\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[양천구]\n",
    "2월 : 0.63\n",
    "3월 : 0.65\n",
    "4월 : 0.64\n",
    "5월 : 0.67\n",
    "6월 : 0.65\n",
    "7월 : 0.69\n",
    "8월 : 0.71\n",
    "9월 : 0.68\n",
    "10월 : 0.69\n",
    "11월 : 0.67\n",
    "12월 : 0.68\n",
    "\n",
    "양천구 연평균 유산도 분석 결과 : 0.67\n",
    "전체 데이터 소수점 2자리까리 반올림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd72fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양천구 유사도 분석 결과\n",
      "데이터 결과 소수점 2자리까지 반올림\n",
      "------------------------------\n",
      "2월 유사도 검증 결과 : 0.63\n",
      "3월 유사도 검증 결과 : 0.65\n",
      "4월 유사도 검증 결과 : 0.64\n",
      "5월 유사도 검증 결과 : 0.67\n",
      "6월 유사도 검증 결과 : 0.65\n",
      "7월 유사도 검증 결과 : 0.69\n",
      "8월 유사도 검증 결과 : 0.71\n",
      "9월 유사도 검증 결과 : 0.68\n",
      "10월 유사도 검증 결과 : 0.69\n",
      "11월 유사도 검증 결과 : 0.67\n",
      "12월 유사도 검증 결과 : 0.68\n",
      "------------------------------\n",
      "양천구 연평균 검증 유사도 결과 : 0.67\n"
     ]
    }
   ],
   "source": [
    "print(\"양천구 유사도 분석 결과\")\n",
    "print(\"데이터 결과 소수점 2자리까지 반올림\")\n",
    "print(\"-\" * 30)\n",
    "print(\"2월 유사도 검증 결과 : 0.63\")\n",
    "print(\"3월 유사도 검증 결과 : 0.65\")\n",
    "print(\"4월 유사도 검증 결과 : 0.64\")\n",
    "print(\"5월 유사도 검증 결과 : 0.67\")\n",
    "print(\"6월 유사도 검증 결과 : 0.65\")\n",
    "print(\"7월 유사도 검증 결과 : 0.69\")\n",
    "print(\"8월 유사도 검증 결과 : 0.71\")\n",
    "print(\"9월 유사도 검증 결과 : 0.68\")\n",
    "print(\"10월 유사도 검증 결과 : 0.69\")\n",
    "print(\"11월 유사도 검증 결과 : 0.67\")\n",
    "print(\"12월 유사도 검증 결과 : 0.68\")\n",
    "print(\"-\" * 30)\n",
    "print(\"양천구 연평균 검증 유사도 결과 : 0.67\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71a21bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1359614655.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    word = \"path = \"./drive/MyDrive/INFO_BOOK_FINAL.csv\"\\\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word = \"path = \"./drive/MyDrive/INFO_BOOK_FINAL.csv\\\n",
    "\\\n",
    "\\\n",
    "chunk_size = 3000000\\\n",
    "\\\n",
    "for cnt, chunk in enumerate(pd.read_csv(path, chunksize=chunk_size)):\\\n",
    "  chunk = chunk[['CTRL_NO', \"LBRRY_CD\", \"TITLE_NM\", \"KDC_NM\", \"ISBN_THIRTEEN_ORGT_NO\", \"CL_SMBL_NO\"]]\\\n",
    "  chunk.to_csv(f\"./drive/MyDrive/df_{cnt}.csv\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./drive/MyDrive/INFO_BOOK_FINAL.csv\"\n",
    "\n",
    "\n",
    "chunk_size = 3000000\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_csv(path, chunksize=chunk_size)):\n",
    "  chunk = chunk[['CTRL_NO', \"LBRRY_CD\", \"TITLE_NM\", \"KDC_NM\", \"ISBN_THIRTEEN_ORGT_NO\", \"CL_SMBL_NO\"]]\n",
    "  chunk.to_csv(f\"./drive/MyDrive/df_{cnt}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58315916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOR CNT, CHUNK IN ENUMERATE(PD.READ_CSV(PATH, CHUNKSIZE=CHUNK_SIZE)):'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'for cnt, chunk in enumerate(pd.read_csv(path, chunksize=chunk_size)):'\n",
    "word.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02664c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
